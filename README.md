# Distributed Job Scheduling System - Anish Shenoy

### Basic File Structure ###
    
    -- coordinator -> single container
        -- app
            -- api.py -> FastAPI structure and query entry points for coordinator
            -- manager.py -> Celery manager/helper and routing to worker
            
    -- worker_nodes -> each worker (3 specified in docker-compose.yaml) has a container
        -- app
            -- celery_tasks.py -> actual definition of executable tasks (ECI trajectory to ECEF trajectory tool)
            -- celery_worker.py -> definition of Celery configs, redis broker definition
            -- worker_utils.py -> utilities to help worker report status to coordinator 
                                  (register_worker function is commanded by docker-compose.yaml 
                                   to verify worker is available at container startup)

### How to start the system ###

With repository as current directory, run:

    % start_services.sh

This will:
- Tear down any currently running containers
- Build containers:
    - Start the Redis broker
    - Start the coordinator: Start FastAPI app using Uvicorn (coordinator will start at port 8000)
    - Start workers _1, _2, and _3: Start workers as specified by Celery configs in app.celery_worker, then python command the register_worker() function to register with the coordinator
           
### How check worker and coordinator logs ###

To check logs (for statuses of workers via the coordinator, or individual status logging via the workers), use docker-compose:

    % docker-compose logs <container-name>

### How run all verification tests ###

All tests (facilitated by Pytest) are located in the /tests folder. CD to this folder and run the `test_cases.py` file. This will execute the following tests:
- Test single job submission
- Check job tracking via coordinator
- Test worker registration entry point to coordinator
- Test concurrent job submission capability

### Job ID vs Task ID ###

Task IDs are generated by the Celery `send_task` method when forwarding job inputs to the actual task executable. Since the workers do not locally register this Task ID, a Job ID is created to track which worker picks up which job.

- Job ID lets you track which worker a task was sent to (found via POST response for `submit-eci-to-ecef` job)
- Task ID lets you track the overall status of the job through the coordinator (via GET response for `job-status` query)
